{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import BCELoss\n",
    "import torchvision.transforms as T\n",
    "from generator import BasicToRifeGenerator, RifeToBasicGenerator, UpscalingGenerator\n",
    "from dataset import Vimeo90KDataset\n",
    "from losses import CharbonnierLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "char_loss = CharbonnierLoss()\n",
    "bce = BCELoss()\n",
    "spynet_path = \"model/spynet_sintel_final-3d2a1287.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    'name': 'GeneratorSweeps',\n",
    "    'method':'bayes',\n",
    "    'metric':{\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize',\n",
    "    },\n",
    "    'parameters':{\n",
    "        'models':{\n",
    "            'values': ['B2R', 'R2B', 'UPGEN'],\n",
    "        }\n",
    "        'learning_rate':{\n",
    "            'distribution': 'log_uniform',\n",
    "            'min': math.log(1e-5),\n",
    "            'max': math.log(1e-3),\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='final_sem', entity='bijin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([T.RandomCrop(224, 224),\n",
    "                        T.RandomHorizontalFlip(),\n",
    "                        T.RandomVerticalFlip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, disc, gen, gen_opt,disc_opt):\n",
    "    \n",
    "    real_labels = torch.ones(x.size(0),1).to(device)\n",
    "    fake_labels = torch.zeros(x.size(0),1).to(device)\n",
    "    disc_opt.zero_grad()\n",
    "    gen_y = gen(x)\n",
    "    real_loss = bce(disc(y),real_labels)\n",
    "    fake_loss = bce(disc(gen_y.detach()),fake_labels)\n",
    "    disc_loss = (real_loss + fake_loss) \n",
    "    disc_loss.backward()\n",
    "    disc_opt.step()\n",
    "    \n",
    "    gen_opt.zero_grad()\n",
    "    \n",
    "    \n",
    "    \n",
    "    gen_loss = char_loss.forward(disc(gen_y),real_labels)\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "    \n",
    "    #put each train step here\n",
    "    #no need to return loss, train loss can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(x, y, disc, gen):\n",
    "    with torch.no_grad:\n",
    "        real_labels = torch.ones(x.size(0),1).to(device)\n",
    "        fake_labels = torch.zeros(x.size(0),1).to(device)\n",
    "        gen_y = gen(x)\n",
    "    \n",
    "        real_loss = bce(disc(y),real_labels)\n",
    "        fake_loss = bce(disc(gen_y.detach()),fake_labels)\n",
    "        disc_loss = (real_loss + fake_loss) \n",
    "        \n",
    "\n",
    "        gen_loss = char_loss.forward(disc(gen_y),real_labels)\n",
    "    return gen_loss, disc_loss\n",
    "    #do validation here\n",
    "    #return val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config=None):\n",
    "    if config.generator == \"B2R\":\n",
    "        gen = BasicToRifeGenerator(spynet_path)\n",
    "    elif config.generator == \"R2B\":\n",
    "        gen = RifeToBasicGenerator(spynet_path)\n",
    "    else:\n",
    "        gen = UpscalingGenerator()\n",
    "    disc = None #add code for tecogan discriminator\n",
    "    return gen, disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_opt(disc, gen, config=None):\n",
    "    gen_opt = AdamW(gen.parameters(), lr=config.learning_rate)\n",
    "    disc_opt = AdamW(disc.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    gen_schedule = CosineAnnealingLR(gen_opt, T_max=300)#check rife paper for T_max\n",
    "    disc_schedule = CosineAnnealingLR(disc_opt, T_max=300)#same here\n",
    "    return gen_opt, disc_opt, gen_schedule, disc_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(config) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        \"\"\"get dataset and dataloaders for train and val\"\"\"\n",
    "        gen, disc = build_model(config)\n",
    "        gen_opt, disc_opt, gen_schedule, disc_schedule = build_opt(disc, gen, config)\n",
    "\n",
    "        for ep in range(EPOCHS):\n",
    "            pass\n",
    "            #do training\n",
    "            #get train samples, do train_steps\n",
    "            #get val samples, do val_steps, get val_loss \n",
    "            #maybe multiple val steps and get mean of loss?\n",
    "            #wandb.log({\"val_loss\": val_loss})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9808e5e21ddde3faa781f9d1a83a7a4b02cf88922b9f4440f597a84ab4fe0f98"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('finalsem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
