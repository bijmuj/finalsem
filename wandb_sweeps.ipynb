{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import BCELoss\n",
    "import torchvision.transforms as T\n",
    "from generator import BasicToRifeGenerator, RifeToBasicGenerator, UpscalingGenerator\n",
    "from dataset import Vimeo90KDataset\n",
    "from torchvision import datasets\n",
    "from losses import CharbonnierLoss\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "DATASET_PATH = ''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "char_loss = CharbonnierLoss()\n",
    "bce = BCELoss()\n",
    "spynet_path = \"model/spynet_sintel_final-3d2a1287.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    'name': 'GeneratorSweeps',\n",
    "    'method':'bayes',\n",
    "    'metric':{\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize',\n",
    "    },\n",
    "    'parameters':{\n",
    "        'models':{\n",
    "            'values': ['B2R', 'R2B', 'UPGEN'],\n",
    "        }\n",
    "        'learning_rate':{\n",
    "            'distribution': 'log_uniform',\n",
    "            'min': math.log(1e-5),\n",
    "            'max': math.log(1e-3),\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='final_sem', entity='bijin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([T.RandomCrop(224, 224),\n",
    "                        T.RandomHorizontalFlip(),\n",
    "                        T.RandomVerticalFlip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, disc, gen, gen_opt,disc_opt):\n",
    "    \n",
    "    real_labels = torch.ones(x.size(0),1).to(device)\n",
    "    fake_labels = torch.zeros(x.size(0),1).to(device)\n",
    "    disc_opt.zero_grad()\n",
    "    gen_y = gen(x)\n",
    "    real_loss = bce(disc(y),real_labels)\n",
    "    fake_loss = bce(disc(gen_y.detach()),fake_labels)\n",
    "    disc_loss = (real_loss + fake_loss) \n",
    "    disc_loss.backward()\n",
    "    disc_opt.step()\n",
    "    \n",
    "    gen_opt.zero_grad()\n",
    "    \n",
    "    \n",
    "    \n",
    "    gen_loss = char_loss.forward(disc(gen_y),real_labels)\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "    \n",
    "    #put each train step here\n",
    "    #no need to return loss, train loss can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(x, y, disc, gen):\n",
    "    with torch.no_grad:\n",
    "        real_labels = torch.ones(x.size(0),1).to(device)\n",
    "        fake_labels = torch.zeros(x.size(0),1).to(device)\n",
    "        gen_y = gen(x)\n",
    "    \n",
    "        real_loss = bce(disc(y),real_labels)\n",
    "        fake_loss = bce(disc(gen_y.detach()),fake_labels)\n",
    "        disc_loss = (real_loss + fake_loss) \n",
    "        \n",
    "\n",
    "        gen_loss = char_loss.forward(disc(gen_y),real_labels)\n",
    "    return gen_loss, disc_loss\n",
    "    #do validation here\n",
    "    #return val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config=None):\n",
    "    if config.generator == \"B2R\":\n",
    "        gen = BasicToRifeGenerator(spynet_path)\n",
    "    elif config.generator == \"R2B\":\n",
    "        gen = RifeToBasicGenerator(spynet_path)\n",
    "    else:\n",
    "        gen = UpscalingGenerator()\n",
    "    disc = None #add code for tecogan discriminator\n",
    "    return gen, disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_opt(disc, gen, config=None):\n",
    "    gen_opt = AdamW(gen.parameters(), lr=config.learning_rate)\n",
    "    disc_opt = AdamW(disc.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    gen_schedule = CosineAnnealingLR(gen_opt, T_max=300)#check rife paper for T_max\n",
    "    disc_schedule = CosineAnnealingLR(disc_opt, T_max=300)#same here\n",
    "    return gen_opt, disc_opt, gen_schedule, disc_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(config) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        \"\"\"get dataset and dataloaders for train and val\"\"\"\n",
    "        gen, disc = build_model(config)\n",
    "        gen_opt, disc_opt, gen_schedule, disc_schedule = build_opt(disc, gen, config)\n",
    "        \n",
    "        train_dataset = datasets.V(root=DATASET_PATH, split='train', download=True, transform=transforms)\n",
    "        num_train = len(train_dataset)\n",
    "        indices = list(range(num_train))\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(np.floor(0.2 * num_train))\n",
    "        train_idx, valid_idx = indices[split:], indices[:split]\n",
    "        \n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,sampler=train_sampler,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset=train_dataset,sampler=valid_sampler,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "        for ep in range(EPOCHS):\n",
    "            valid_gen_loss = 0.0\n",
    "            valid_disc_loss = 0.0\n",
    "            gen.train()\n",
    "            for (x, y) in enumerate(train_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                train_step(x,y,disc,gen,gen_opt,disc_opt)\n",
    "            gen.eval()    \n",
    "                \n",
    "            for (x,y) in enumerate(valid_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                gen_loss,disc_loss = val_step(x,y,disc,gen)\n",
    "                valid_gen_loss += gen_loss\n",
    "                valid_disc_loss += disc_loss\n",
    "            valid_gen_loss = valid_gen_loss/len(valid_loader.sampler)\n",
    "            valid_disc_loss = valid_disc_loss/len(valid_loader.sampler)    \n",
    "            print(f'Gen Loss = {valid_gen_loss} Disc Loss = {valid_disc_loss}')\n",
    "            \n",
    "            #do training\n",
    "            #get train samples, do train_steps\n",
    "            #get val samples, do val_steps, get val_loss \n",
    "            #maybe multiple val steps and get mean of loss?\n",
    "            #wandb.log({\"val_loss\": val_loss})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9808e5e21ddde3faa781f9d1a83a7a4b02cf88922b9f4440f597a84ab4fe0f98"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('finalsem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
